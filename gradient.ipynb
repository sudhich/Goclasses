{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f48991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at x=4.0, f(x)=16.0\n",
      "Step 1: x=3.2000, f(x)=10.2400, grad=8.0000\n",
      "Step 2: x=2.5600, f(x)=6.5536, grad=6.4000\n",
      "Step 3: x=2.0480, f(x)=4.1943, grad=5.1200\n",
      "Step 4: x=1.6384, f(x)=2.6844, grad=4.0960\n",
      "Step 5: x=1.3107, f(x)=1.7180, grad=3.2768\n",
      "Step 6: x=1.0486, f(x)=1.0995, grad=2.6214\n",
      "Step 7: x=0.8389, f(x)=0.7037, grad=2.0972\n",
      "Step 8: x=0.6711, f(x)=0.4504, grad=1.6777\n",
      "Step 9: x=0.5369, f(x)=0.2882, grad=1.3422\n",
      "Step 10: x=0.4295, f(x)=0.1845, grad=1.0737\n",
      "Step 11: x=0.3436, f(x)=0.1181, grad=0.8590\n",
      "Step 12: x=0.2749, f(x)=0.0756, grad=0.6872\n",
      "Step 13: x=0.2199, f(x)=0.0484, grad=0.5498\n",
      "Step 14: x=0.1759, f(x)=0.0309, grad=0.4398\n",
      "Step 15: x=0.1407, f(x)=0.0198, grad=0.3518\n",
      "Step 16: x=0.1126, f(x)=0.0127, grad=0.2815\n",
      "Step 17: x=0.0901, f(x)=0.0081, grad=0.2252\n",
      "Step 18: x=0.0721, f(x)=0.0052, grad=0.1801\n",
      "Step 19: x=0.0576, f(x)=0.0033, grad=0.1441\n",
      "Step 20: x=0.0461, f(x)=0.0021, grad=0.1153\n",
      "Step 21: x=0.0369, f(x)=0.0014, grad=0.0922\n",
      "Step 22: x=0.0295, f(x)=0.0009, grad=0.0738\n",
      "Step 23: x=0.0236, f(x)=0.0006, grad=0.0590\n",
      "Step 24: x=0.0189, f(x)=0.0004, grad=0.0472\n",
      "Step 25: x=0.0151, f(x)=0.0002, grad=0.0378\n",
      "Step 26: x=0.0121, f(x)=0.0001, grad=0.0302\n",
      "Step 27: x=0.0097, f(x)=0.0001, grad=0.0242\n",
      "Step 28: x=0.0077, f(x)=0.0001, grad=0.0193\n",
      "Step 29: x=0.0062, f(x)=0.0000, grad=0.0155\n",
      "Step 30: x=0.0050, f(x)=0.0000, grad=0.0124\n",
      "Step 31: x=0.0040, f(x)=0.0000, grad=0.0099\n",
      "Step 32: x=0.0032, f(x)=0.0000, grad=0.0079\n",
      "Step 33: x=0.0025, f(x)=0.0000, grad=0.0063\n",
      "Step 34: x=0.0020, f(x)=0.0000, grad=0.0051\n",
      "Step 35: x=0.0016, f(x)=0.0000, grad=0.0041\n",
      "Step 36: x=0.0013, f(x)=0.0000, grad=0.0032\n",
      "Step 37: x=0.0010, f(x)=0.0000, grad=0.0026\n",
      "Step 38: x=0.0008, f(x)=0.0000, grad=0.0021\n",
      "Step 39: x=0.0007, f(x)=0.0000, grad=0.0017\n",
      "Step 40: x=0.0005, f(x)=0.0000, grad=0.0013\n",
      "Step 41: x=0.0004, f(x)=0.0000, grad=0.0011\n",
      "Step 42: x=0.0003, f(x)=0.0000, grad=0.0009\n",
      "Converged after 42 steps!\n",
      "Final minimum at x≈0.0003402823669209386, f(x)≈1.1579208923731628e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def gradient(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Parameters\n",
    "x = 4.0  # Initial guess\n",
    "learning_rate = 0.1\n",
    "precision = 0.0001  # Stop when change < this\n",
    "max_steps = 100\n",
    "step = 0\n",
    "\n",
    "print(f\"Starting at x={x}, f(x)={f(x)}\")\n",
    "\n",
    "while step < max_steps:\n",
    "    prev_x = x\n",
    "    grad = gradient(x)\n",
    "    x = x - learning_rate * grad\n",
    "    step += 1\n",
    "    \n",
    "    print(f\"Step {step}: x={x:.4f}, f(x)={f(x):.4f}, grad={grad:.4f}\")\n",
    "    \n",
    "    if abs(x - prev_x) < precision:\n",
    "        print(f\"Converged after {step} steps!\")\n",
    "        break\n",
    "\n",
    "print(f\"Final minimum at x≈{x}, f(x)≈{f(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9f8508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at x=4.000000, f(x)=18.180710\n",
      "Step 1: x=2.590965, f(x)=3.800107, grad=14.090355\n",
      "Step 2: x=1.938533, f(x)=0.548945, grad=6.524317\n",
      "Step 3: x=1.588044, f(x)=-0.421664, grad=3.504884\n",
      "Step 4: x=1.382345, f(x)=-0.763639, grad=2.056996\n",
      "Step 5: x=1.254595, f(x)=-0.897590, grad=1.277499\n",
      "Step 6: x=1.172224, f(x)=-0.953874, grad=0.823711\n",
      "Step 7: x=1.117748, f(x)=-0.978674, grad=0.544763\n",
      "Step 8: x=1.081088, f(x)=-0.989963, grad=0.366593\n",
      "Step 9: x=1.056121, f(x)=-0.995217, grad=0.249669\n",
      "Step 10: x=1.038976, f(x)=-0.997702, grad=0.171456\n",
      "Step 11: x=1.027133, f(x)=-0.998889, grad=0.118427\n",
      "Step 12: x=1.018920, f(x)=-0.999461, grad=0.082129\n",
      "Step 13: x=1.013209, f(x)=-0.999738, grad=0.057116\n",
      "Step 14: x=1.009229, f(x)=-0.999872, grad=0.039799\n",
      "Step 15: x=1.006452, f(x)=-0.999937, grad=0.027771\n",
      "Step 16: x=1.004512, f(x)=-0.999969, grad=0.019396\n",
      "Step 17: x=1.003156, f(x)=-0.999985, grad=0.013556\n",
      "Step 18: x=1.002208, f(x)=-0.999993, grad=0.009479\n",
      "Step 19: x=1.001545, f(x)=-0.999996, grad=0.006630\n",
      "Step 20: x=1.001082, f(x)=-0.999998, grad=0.004639\n",
      "Step 21: x=1.000757, f(x)=-0.999999, grad=0.003246\n",
      "Step 22: x=1.000530, f(x)=-1.000000, grad=0.002271\n",
      "Step 23: x=1.000371, f(x)=-1.000000, grad=0.001590\n",
      "Step 24: x=1.000260, f(x)=-1.000000, grad=0.001113\n",
      "Step 25: x=1.000182, f(x)=-1.000000, grad=0.000779\n",
      "Converged after 25 steps!\n",
      "Final minimum at x≈1.000182, f(x)≈-1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return (x**2) * np.log(x) - x\n",
    "\n",
    "def gradient(x):\n",
    "    return (2*x) * np.log(x) + x - 1\n",
    "\n",
    "# Parameters\n",
    "x = 4.0  # Initial guess\n",
    "learning_rate = 0.1\n",
    "precision = 0.0001  # Stop when change < this\n",
    "max_steps = 100\n",
    "step = 0\n",
    "\n",
    "print(f\"Starting at x={x:.6f}, f(x)={f(x):.6f}\")\n",
    "\n",
    "while step < max_steps:\n",
    "    prev_x = x\n",
    "    grad = gradient(x)\n",
    "    x_new = x - learning_rate * grad\n",
    "    \n",
    "    # Ensure x stays positive to avoid log(0) or negative\n",
    "    if x_new <= 0:\n",
    "        print(f\"Step {step+1}: x became non-positive ({x_new:.6f}), stopping to avoid log error.\")\n",
    "        break\n",
    "    \n",
    "    x = x_new\n",
    "    step += 1\n",
    "    \n",
    "    print(f\"Step {step}: x={x:.6f}, f(x)={f(x):.6f}, grad={grad:.6f}\")\n",
    "    \n",
    "    if abs(x - prev_x) < precision:\n",
    "        print(f\"Converged after {step} steps!\")\n",
    "        break\n",
    "\n",
    "print(f\"Final minimum at x≈{x:.6f}, f(x)≈{f(x):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd25db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
